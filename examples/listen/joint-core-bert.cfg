
# Training hyper-parameters and additional features.
[training]
# Whether to train on sequences with 'gold standard' sentence boundaries
# and tokens. If you set this to true, take care to ensure your run-time
# data is passed in sentence-by-sentence via some prior preprocessing.
gold_preproc = true
# Limitations on training document length or number of examples.
max_length = 1000
limit = 0
# Data augmentation
orth_variant_level = 0.0
dropout = 0.1
# Controls early-stopping.
patience = 10000
eval_frequency = 200
max_epochs = 10000
# Other settings
seed = 0
accumulate_gradient = 2
width = 768
# Control how scores are printed and checkpoints are evaluated.
scores = ["speed", "tags_acc", "uas", "las", "ents_f"]
score_weights = {"las": 0.4, "ents_f": 0.4, "tags_acc": 0.2}
# These settings are invalid for the transformer models.
init_tok2vec = null
vectors = null


[training.batch_size]
@schedules = "compounding.v1"
start = 500
stop = 500
compound = 1.001

[optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = true
eps = 1e-8
initial_rate = 5e-5

[optimizer.learn_rate]
@schedules = "warmup_linear.v1"
warmup_steps = 250
total_steps = 20000

[nlp]
lang = "en"
vectors = ${training:vectors}

# Define the pipeline. The train-from-config command looks up the factories
# and assembles the NLP object. You can register more factories using the
# @component decorator.

[nlp.pipeline.transformer]
# This component holds the actual transformer (wrapped via pytorch). The 
# transformer is run once and then the predictions are used by the other
# components. Weights are updated from multi-task gradients.
factory = "transformer"

[nlp.pipeline.tagger]
factory = "tagger"

[nlp.pipeline.parser]
factory = "parser"

[nlp.pipeline.ner]
factory = "ner"


# This loads the Huggingface Transformers model. The transformer is applied
# to a batch of Doc objects, which are preprocessed into Span objects to support
# longer documents.
[nlp.pipeline.transformer.model]
@architectures = "spacy.TransformerByName.v2"
name = "bert-base-cased"
fast_tokenizer = true

[nlp.pipeline.transformer.model.get_spans]
# You can set a custom strategy for preparing spans from the batch, e.g. you
# can predict over sentences. Here we predict over the whole document.
@layers = "spacy-transformers.strided_spans.v1"
window = 128
stride = 128

[nlp.pipeline.tagger.model]
@architectures = "spacy.Tagger.v1"

[nlp.pipeline.parser.model]
@architectures = "spacy.TransitionBasedParser.v1"
nr_feature_tokens = 8
hidden_width = 128
maxout_pieces = 3
use_upper = false

[nlp.pipeline.ner.model]
@architectures = "spacy.TransitionBasedParser.v1"
nr_feature_tokens = 3
hidden_width = 128
maxout_pieces = 3
use_upper = false

# These "listener" layers are connected to the transformer pipeline component
# in order to achieve multi-task learning across the pipeline.
# They rely on the transformer to predict over the batch and cache the result
# and callback. The gradient for the transformers will be accumulated by
# the listeners, and then the last listener will call the backprop callback.
[nlp.pipeline.tagger.model.tok2vec]
@architectures = "spacy.Tok2VecTransformerListener.v1"
width = ${training:width}
grad_factor = 1.0

[nlp.pipeline.parser.model.tok2vec]
@architectures = "spacy.Tok2VecTransformerListener.v1"
width = ${training:width}
grad_factor = 1.0

[nlp.pipeline.ner.model.tok2vec]
@architectures = "spacy.Tok2VecTransformerListener.v1"
width = ${training:width}
grad_factor = 1.0

# These pooling layers control how the token vectors are calculated from
# the word pieces. The reduce_mean layer averages the wordpieces, so if you
# have one token aligned to multiple wordpieces (as is expected), the token's
# vector will be the average of the wordpieces. The most obvious alternative
# is reduce_last.v1, which would just use the last wordpiece. You could also
# try reduce_first, reduce_sum or even reduce_max.

[nlp.pipeline.tagger.model.tok2vec.pooling]
@layers = "reduce_mean.v1"

[nlp.pipeline.parser.model.tok2vec.pooling]
@layers = "reduce_mean.v1"

[nlp.pipeline.ner.model.tok2vec.pooling]
@layers = "reduce_mean.v1"
